# Day 2 - Predicting Tesla Stock Prices with RNNs - Shrasti Gupta

This project is my hands-on exercise to dive into the world of Recurrent Neural Networks (RNNs). My goal was to build and train an LSTM model to see if I could predict the next-day opening price of Tesla's stock (TSLA) using historical data from Kaggle.

## My Project's Results: A Quick Look

After training my model, I plotted its predictions (in blue) against the actual stock prices (in red) for the test period. It was really cool to see how well the model managed to capture the overall trend of the stock price!

### Screenshot of My Prediction Plot

*(This is where you should add a screenshot of the graph generated by the final cell of the notebook. It will look something like this.)*

![Tesla Stock Prediction Plot](Screenshots/image.png)

As you can see, the predicted values follow the real values pretty closely, which tells me the model learned the basic patterns in the data.

---

## How to Get This Running on Your Machine

If you want to run this project yourself, hereâ€™s how you can do it.

### 1. Prerequisites
Make sure you have Python 3.8+ installed on your system.

### 2. Setup
First, clone this repository to your local machine.

```bash
git clone https://github.com/Shrasti0207/pi-shaped-deep-learning-shrastigupta.git
cd pi-shaped-deep-learning-Shrasti
```


```bash
python -m venv my-rnn-env
source ~/jupyter-env/bin/activate ( my virtual env)
```

Then, install all the necessary libraries.

```bash
pip install -r requirements.txt
```
```
pandas
scikit-learn
tensorflow
matplotlib
numpy
```

### 3. Get the Dataset
Download the Tesla Stock Data from Kaggle: [Tesla Stock Data (TSLA)](https://www.kaggle.com/datasets/rpaguirregabiria/tesla-stock-data-updated-till-2023)


### 4. Run the Code
Now you're all set! Just launch Jupyter Notebook or JupyterLab and open the `.ipynb` file to run the code cell by cell.

```bash
jupyter notebook
```

---

## My Answers to Core Concept Questions

During this exercise, I had to think about a few key ideas. Here are my thoughts on them.

#### What's the big deal with using RNNs (or LSTMs) for time-series data?
RNNs (and especially LSTMs) can remember past inputs through hidden states, making them ideal for sequential data like time series. Feedforward networks treat each input independently, missing temporal relationships. LSTMs further solve the vanishing gradient problem, enabling them to capture long-term dependencies.

#### Why is making those input "windows" so important?
Time-series forecasting requires past context to predict the future. Input windows (sequence framing) allow the model to see a slice of past data and learn patterns across time. Without framing, the model only sees single points and cannot learn temporal dependencies.

#### How does scaling the data actually help my RNN/LSTM model?
Scaling (e.g., MinMax or StandardScaler) ensures all features are on a similar range, preventing large-valued features from dominating. It speeds up convergence during training and stabilizes gradient updates, especially in RNNs/LSTMs where values propagate across time steps.

#### What's the difference between a SimpleRNN and an LSTM for long-term patterns?
SimpleRNNs struggle with long-term dependencies due to vanishing gradients. LSTMs introduce memory cells and gates (input, forget, output) to retain information over long sequences. Thus, LSTMs perform much better for tasks requiring long history context.

#### Which metrics like MAE or RMSE are good for this, and why?
FMAE (Mean Absolute Error) shows average absolute deviation, easy to interpret in currency units. RMSE (Root Mean Squared Error) penalizes larger errors more, making it useful for stock prediction where big mistakes can be costly. Both are common in financial forecasting.

#### How can I tell if my model is overfitting?
Overfitting occurs when training accuracy is much higher than validation/test accuracy. You can monitor learning curves (loss vs epochs), or use cross-validation. High variance between train and test performance is a clear sign.

#### How could I make this model even better?
You can add more features (e.g., volume, technical indicators, news sentiment), use deeper or bidirectional networks, or combine RNN/LSTM with CNNs or attention mechanisms. Regularization and hyperparameter tuning also improve generalization.

#### Why is it so important *not* to shuffle sequential data?
Shuffling breaks temporal order, which destroys patterns in time series. For time series forecasting, you must keep data sequential to preserve dependencies. However, within mini-batches of sequential slices, careful shuffling can help avoid bias.

#### How can I visualize the results to see how my model performed?
Plotting predicted vs actual values on the same time axis gives a clear view of how closely the model tracks reality. Residual plots (errors over time) also help identify systematic biases. This visual comparison helps interpret model reliability.

#### What are the real-world problems with using RNNs for stock prediction?
Stock prices are influenced by unpredictable external factors (news, politics, global events). Data is noisy, non-stationary, and sometimes lacks enough historical context. Overfitting and poor generalization are common, making robust and interpretable models difficult to build.